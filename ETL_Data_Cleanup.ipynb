{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-61da716feaf0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import csv  \n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Kaggle CSV With Cases & Deaths By State #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kaggle csv file with total cases and deaths by US state and county\n",
    "# Data was downloaded from https://www.kaggle.com/imdevskp/corona-virus-report#usa_county_wise.csv\n",
    "file = \"Raw_Data/usa_county_wise.csv\"\n",
    "kaggle_df = pd.read_csv(file)\n",
    "kaggle_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows for US states (i.e. remove Guam, Virgin Islands, etc.)\n",
    "kaggle_us = kaggle_df.loc[kaggle_df[\"iso2\"] == \"US\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns not needed (UID, iso2(country/territory), iso3(country/territory), & code3)\n",
    "kaggle_rem_cols = kaggle_us[[\"FIPS\", \"Admin2\",\"Province_State\", \"Lat\", \"Long_\", \"Combined_Key\", \"Date\", \"Confirmed\", \"Deaths\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns (Admin 2 to County, Province_State to State, Long_ to Lng)\n",
    "kaggle_renamed = kaggle_rem_cols.rename(columns={\"Admin2\":\"County\", \"Province_State\":\"State\", \"Long_\":\"Lng\"})\n",
    "kaggle_renamed.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "kaggle_renamed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data\n",
    "kaggle_drop = kaggle_renamed.dropna()\n",
    "kaggle_drop.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if date column date are strings or date objects\n",
    "kaggle_drop.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dates from strings to datetime objects\n",
    "kaggle_drop['Date'] = pd.to_datetime(kaggle_drop['Date'],format='%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that column type changed\n",
    "kaggle_drop.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows without a county name\n",
    "kaggle_final = kaggle_drop.loc[kaggle_drop[\"County\"] != \"Unassigned\",:]\n",
    "kaggle_final2 = kaggle_final[~kaggle_final[\"County\"].str.contains(\"Out of\")]\n",
    "kaggle_final2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to use as primary key for county table\n",
    "kaggle_final3 = kaggle_final2.reset_index(drop=True)\n",
    "kaggle_final3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export kaggle df as a csv so it can be imported to postgres\n",
    "kaggle_final3.to_csv(\"Clean_CSVs/county_data.csv\", encoding=\"utf-8\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import CDC CSV With COVID Forecasts By State #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv file with forecast of potential deaths by state\n",
    "# Data was downloaded from https://www.cdc.gov/coronavirus/2019-ncov/covid-data/forecasting-us.html\n",
    "file2 = \"Raw_Data/forecast_data_0413.csv\"\n",
    "forecast_raw = pd.read_csv(file2)\n",
    "forecast_raw.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows for states, not whole of US\n",
    "forecast_states = forecast_raw.loc[forecast_raw[\"location_name\"] != \"US\",:]\n",
    "forecast_states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing data\n",
    "forecast_states = forecast_states.dropna()\n",
    "forecast_states.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns (target week end date to target end date, location name to state, point to actual)\n",
    "forecast_renamed = forecast_states.rename(columns={\"target_week_end_date\":\"target_end_date\", \"location_name\":\"state\", \"point\":\"actual\"})\n",
    "forecast_renamed.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if date column types are datetime or string \n",
    "forecast_renamed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dates from strings to datetime objects\n",
    "forecast_renamed['forecast_date'] = pd.to_datetime(forecast_renamed['forecast_date'],format='%m/%d/%Y')\n",
    "forecast_renamed['target_end_date'] = pd.to_datetime(forecast_renamed['target_end_date'],format='%m/%d/%Y')\n",
    "forecast_renamed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify columns changed to datetime\n",
    "forecast_renamed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to use as primary key for county table\n",
    "forecast_final = forecast_renamed.reset_index(drop=True)\n",
    "forecast_final.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export forecast df as a csv so it can be imported to postgres\n",
    "forecast_final.to_csv(\"Clean_CSVs/forecast_cdc.csv\", encoding=\"utf-8\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Policydates And Hospital Resource Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define empty lists to store data for each state\n",
    "covid19_healthdate_dates_bystate = []\n",
    "covid19_healthdate_resources_bystate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list of all 50 states plus the District of Columbia saved in a\n",
    "#variable named us_states_list\n",
    "\n",
    "us_states_list = [\n",
    "    'Alabama',\n",
    "    'Alaska',\n",
    "    'Arizona',\n",
    "    'Arkansas',\n",
    "    'California',\n",
    "    'Colorado',\n",
    "    'Connecticut',\n",
    "    'Delaware',\n",
    "    'District of Columbia',\n",
    "    'Florida',\n",
    "    'Georgia',\n",
    "    'Hawaii',\n",
    "    'Idaho',\n",
    "    'Illinois',\n",
    "    'Indiana',\n",
    "    'Iowa',\n",
    "    'Kansas',\n",
    "    'Kentucky',\n",
    "    'Louisiana',\n",
    "    'Maine',\n",
    "    'Maryland',\n",
    "    'Massachusetts',\n",
    "    'Michigan',\n",
    "    'Minnesota',\n",
    "    'Mississippi',\n",
    "    'Missouri',\n",
    "    'Montana',\n",
    "    'Nebraska',\n",
    "    'Nevada',\n",
    "    'New Hampshire',\n",
    "    'New Jersey',\n",
    "    'New Mexico',\n",
    "    'New York',\n",
    "    'North Carolina',\n",
    "    'North Dakota',\n",
    "    'Ohio',\n",
    "    'Oklahoma',\n",
    "    'Oregon',\n",
    "    'Pennsylvania',\n",
    "    'Rhode Island',\n",
    "    'South Carolina',\n",
    "    'South Dakota',\n",
    "    'Tennessee',\n",
    "    'Texas',\n",
    "    'Utah',\n",
    "    'Vermont',\n",
    "    'Virginia',\n",
    "    'Washington',\n",
    "    'West Virginia',\n",
    "    'Wisconsin',\n",
    "    'Wyoming'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrape covid19.healthdata.org for each state\n",
    "def covid19_healthdata_scrape(state):\n",
    "    #formating the state to match url \n",
    "    formatedstate = state.lower().replace(' ', '-') \n",
    "    url = f\"https://covid19.healthdata.org/united-states-of-america/{formatedstate}\"\n",
    "    #access the webpage using selenium\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    #after accessing the page, wait 5 seconds so the page can fully load before running next line of code\n",
    "    time.sleep(5)\n",
    "    #location of policy dates information\n",
    "    dates_div = driver.find_elements_by_xpath(\"/html/body/div/div/main/div[3]/div[1]/div[2]\")\n",
    "    #grab the information using list comprehension\n",
    "    dates_divs_text = [x.text for x in dates_div]\n",
    "    #splitting up the data as they can all stored in the same div\n",
    "    dates_text_split = dates_divs_text[0].split('\\n')\n",
    "    #location of resources information\n",
    "    resources_div = driver.find_elements_by_xpath(\"/html/body/div/div/main/div[3]/div[3]/div[2]/div/div[2]/div[2]\")\n",
    "    #grab the information using list comprehension\n",
    "    resources_div_text = [x.text for x in resources_div]\n",
    "    #splitting up the data as they can all stored in the same div\n",
    "    resources_div_split = resources_div_text[0].split('\\n')\n",
    "    #close the web browser after getting the information needed\n",
    "    driver.close()\n",
    "    #splitting the dates data and storing them in a dictionary\n",
    "    dates_data = {\n",
    "    'state': state,\n",
    "    'mass_gathering_restriction': dates_text_split[1],\n",
    "    'initual_business_closure': dates_text_split[3],\n",
    "    'educational_facilities_closure': dates_text_split[5],\n",
    "    'non-Essential_services_closure': dates_text_split[7],\n",
    "    'stay_at_home_order': dates_text_split[9],\n",
    "    'travel_severely_limited': dates_text_split[11]}\n",
    "    #splitting the resources data, removing unnecessary words, converting them into integers, and storing them in a dictionary\n",
    "    resources_data = {\n",
    "    'state': state,\n",
    "    'hospital_beds_needed': int(resources_div_split[1].replace('beds','').replace(',','')),\n",
    "    'hospital_beds_available': int(resources_div_split[3].replace('beds','').replace(',','')),\n",
    "    'hospital_beds_shortage': int(resources_div_split[5].replace('beds','').replace(',','')),\n",
    "    'icu_beds_needed': int(resources_div_split[7].replace('beds','').replace(',','')),\n",
    "    'icu_beds_available': int(resources_div_split[9].replace('beds','').replace(',','')),\n",
    "    'icu_beds_shortage': int(resources_div_split[11].replace('beds','').replace(',','')),\n",
    "    'ventilators_needed': int(resources_div_split[13].replace('ventilators','').replace(',',''))\n",
    "    }\n",
    "    #push both dictionaries into their respective list\n",
    "    covid19_healthdate_dates_bystate.append(dates_data)\n",
    "    covid19_healthdate_resources_bystate.append(resources_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the code to scrape covid19.healthdata site to grab data for the\n",
    "#policy dates declaration and hospital resource information for each state\n",
    "#and save them into list of dictionaries\n",
    "for state in us_states:\n",
    "    covid19_healthdata_scrape(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the policydates from a list of dictionaries to a panda dataframe\n",
    "policydates_df = pd.DataFrame(covid19_healthdate_dates_bystate)\n",
    "#Converting the hospital resources from a list of dictionaries to a panda dataframe\n",
    "resource_df = pd.DataFrame(covid19_healthdate_resources_bystate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the index for both dataframe to the states \n",
    "policydates_df = policydates_df.set_index('state')\n",
    "resource_df = resource_df.set_index('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grabbing the name of each columns from the policydates dataframe \n",
    "columns = policydates_df.columns\n",
    "#Going through each state and changing all of the data strings into datetime #objects but only keeping the date portion of the datetime object\n",
    "#Some states have not implemented some policy so thus has a not implemented\n",
    "#value, those values were converted to an empty space\n",
    "\n",
    "for x in columns:\n",
    "    for y in range(len(us_states)):\n",
    "        try:\n",
    "            policydates_df[x][y] = pd.to_datetime(policydates_df[x][y], format='%B %d, %Y')\n",
    "            policydates_df[x][y] = policydates_df[x][y].date()\n",
    "        except ValueError:\n",
    "            policydates_df[x][y] = policydates_df[x][y].replace('Not implemented', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting the dataframes as csvs \n",
    "policydates_df.to_csv('Clean_CSVs/covid19_policydates.csv')\n",
    "resource_df.to_csv('Clean_CSVs/hospital_resources.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping WorldOMeters #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from pytz import timezone\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.worldometers.info/coronavirus/\n",
    "\n",
    "#https://www.worldometers.info/coronavirus/country/us/\n",
    "\n",
    "\"\"\" Worldometers.info\tWeb scrape\t\n",
    "Total cases, new cases(per day), total deaths, \n",
    "new deaths, active cases, total cases/1M pop, deaths/1M pop, \n",
    "total tests, tests/1M pop\n",
    " \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the requests and retrieving the HTML contents #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the request (needs to be 200)\n",
    "result = requests.get(\"https://www.worldometers.info/coronavirus/country/us/\")\n",
    "\n",
    "src = result.content\n",
    "soup = BeautifulSoup(src, 'html.parser')\n",
    "\n",
    "table = soup.find_all('table')\n",
    "\n",
    "state_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the table data from the HTML and stripping unnecessary characters and tags#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#html by id tag\n",
    "table_data = soup.find(id=\"usa_table_countries_today\")\n",
    "\n",
    "#html by style tags\n",
    "table_data = table_data.find_all(style = [\"font-weight: bold; text-align:right\",\"text-align:right;font-weight:bold;\",\\\n",
    "\"font-weight: bold; text-align:right;\",\"font-weight: bold; text-align:right;background-color:#FFEEAA;\",\"font-weight: bold; text-align:right;background-color:red; color:white\"] )\n",
    "#six_day = table_data.find_all('td')\n",
    "print(table_data)\n",
    "\n",
    "\n",
    "#for loop to strip the tags from the HTML\n",
    "for data in table_data:\n",
    "    \n",
    "    data = data.text\n",
    "    data = data.replace(',', '')\n",
    "    data = data.replace(' ', '')\n",
    "    data = data.strip('\\n')\n",
    "    \n",
    "    state_list.append(data) #stripping the '\\n' from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the row and column lists for the dataframe #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of the states for the dataframe rows\n",
    "list_of_states = ['New York', 'New Jersey', 'Massachusetts', 'California', 'Pennsylvania', 'Illinois', 'Michigan', 'Florida', 'Louisiana', 'Connecticut', 'Texas', 'Georgia', 'Maryland', 'Ohio\\\n",
    "', 'Indiana', 'Washington', 'Colorado', 'Virginia', 'Tennessee', 'North Carolina', 'Missouri', 'Rhode Island', 'Alabama', 'Arizona', 'Mississippi', 'Wisconsin', 'South Carolina', 'Nevada', 'Iowa\\\n",
    "', 'Utah', 'Kentucky', 'District Of Columbia', 'Delaware', 'Oklahoma', 'Minnesota', 'Arkansas', 'Kansas', 'New Mexico', 'Oregon', 'Nebraska', 'South Dakota\\\n",
    "', 'Idaho', 'New Hampshire', 'West Virginia', 'Maine', 'Vermont', 'North Dakota', 'Hawaii', 'Wyoming', 'Montana', 'Alaska']\n",
    "\n",
    "#column list for the dateframe columns\n",
    "list_of_columns = [\"Total_cases\", \"New_cases\", \"Total_deaths\", \"New_deaths\", \"Active_cases\", \"Cases_per_million\", \"Deaths_per_million\", \"Total_tests\", \"Tests_per_million\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = state_list[:459] #intentially cutting string values off to eliminate US territories and crusie ships in the data\n",
    "\n",
    "df = pd.DataFrame(np.array(state_list).reshape(51,9), index= list_of_states, columns = list_of_columns) #turns the giant list of data into a 51x9 dataframe with the columns and rows listed\n",
    "\n",
    "df.index.name = \"State\" #sets index name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the news date and time and stripping tags and characters #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_date = soup.find(style=\"font-size:13px; color:#999; text-align:center\") #find the date and time in GMT time\n",
    "news_date = news_date.text #strip the HTML tags\n",
    "news_date = news_date.strip(\"Last updated: \") # removes the Last Updated part of text\n",
    "\n",
    "\n",
    "#stripping the time string of spaces and colon\n",
    "news_date = news_date.replace(' ', '')\n",
    "news_date = news_date.replace(':', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating CSV file name and loading date and time from HTML into title #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_string = \"states - \" + str(news_date) + \".csv\"\n",
    "\n",
    "#dictionary to convert month name to month number of the year\n",
    "\n",
    "month_string_to_number = {\n",
    "    'January': '01',\n",
    "    'February': '02',\n",
    "    'March': '03',\n",
    "    'April':'04',\n",
    "    'May':'05',\n",
    "    'June':'06',\n",
    "    'July':'07',\n",
    "    'August':'08',\n",
    "    'September':'09',\n",
    "    'October':'10',\n",
    "    'November':'11',\n",
    "    'December':'12'\n",
    "    }\n",
    "\n",
    "\n",
    "#pulling date and time info from news_date string\n",
    "month_string = news_date[:-15]\n",
    "month_number = month_string_to_number[month_string]\n",
    "day_string = int(news_date[-15:-13])\n",
    "year_string = news_date[-12:-8]\n",
    "GMTtime_string = int(news_date[-7:-3])\n",
    "ESTtime_string = year_string + \"-\" + month_number + \"-\" + str(day_string)\n",
    "df.insert(0, 'Date', ESTtime_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stripping the final extraneous characters and creating CSV from dataframe #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Strip the plus signs from the \"new\" columns\n",
    "df['New_cases'] = df['New_cases'].str.replace('+', '')\n",
    "df['New_deaths'] = df['New_deaths'].str.replace('+', '')\n",
    "\n",
    "#creating and uploading dataframe to csv file with the name of the date and time\n",
    "\n",
    "df.to_csv(csv_string)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('PythonData': conda)",
   "language": "python",
   "name": "python37664bitpythondataconda295494d0ace540d685491cb338ac9b34"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
